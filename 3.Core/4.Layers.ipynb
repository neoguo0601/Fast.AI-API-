{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "该模块包含许多我们可能有兴趣在模型中使用的图层类。 这些图层补充了我们也可以用作预定义图层的默认Pytorch图层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom fastai modules\n",
    "<b>class AdaptiveConcatPool2d</b>\n",
    "\n",
    "`AdaptiveConcatPool2d(sz:Optional[int]=None) :: Module`\n",
    "\n",
    "```python\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.\"\n",
    "    def __init__(self, sz:Optional[int]=None):\n",
    "        \"Output will be 2*sz or 2 if sz is None\"\n",
    "        super().__init__()\n",
    "        self.output_size = sz or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.output_size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.output_size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "```\n",
    "\n",
    "连接AdaptiveAvgPool2d和AdaptiveMaxPool2d的层。\n",
    "\n",
    "输出为2 * sz，如果sz为None，则输出为2。\n",
    "\n",
    "AdaptiveConcatPool2d对象使用自适应平均池和自适应最大池并将它们连接起来。 我们使用它是因为它为模型提供了两种方法的信息并提高了性能。 这种技术称为自适应，因为它允许我们决定我们想要的输出尺寸，而不是选择输入的尺寸以适合所需的输出尺寸。\n",
    "\n",
    "让我们首先尝试使用自适应平均池进行训练，然后使用自适应最大池进行训练，最后使用它们的串联来查看它们在性能方面的表现。\n",
    "\n",
    "我们将首先通过稍微更改源代码来使用Adaptive Max Pooling定义simple_cnn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.basics import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_max(actns:Collection[int], kernel_szs:Collection[int]=None,\n",
    "               strides:Collection[int]=None) -> nn.Sequential:\n",
    "    \"CNN with `conv2d_relu` layers defined by `actns`, `kernel_szs` and `strides`\"\n",
    "    nl = len(actns)-1\n",
    "    kernel_szs = ifnone(kernel_szs, [3]*nl)\n",
    "    strides    = ifnone(strides   , [2]*nl)\n",
    "    layers = [conv_layer(actns[i], actns[i+1], kernel_szs[i], stride=strides[i])\n",
    "        for i in range(len(strides))]\n",
    "    layers.append(nn.Sequential(nn.AdaptiveMaxPool2d(1), Flatten()))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.119670</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = simple_cnn_max((3,16,16,2))\n",
    "learner = Learner(data, model, metrics=[accuracy])\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们尝试使用Adaptive Average Pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_avg(actns:Collection[int], kernel_szs:Collection[int]=None,\n",
    "               strides:Collection[int]=None) -> nn.Sequential:\n",
    "    \"CNN with `conv2d_relu` layers defined by `actns`, `kernel_szs` and `strides`\"\n",
    "    nl = len(actns)-1\n",
    "    kernel_szs = ifnone(kernel_szs, [3]*nl)\n",
    "    strides    = ifnone(strides   , [2]*nl)\n",
    "    layers = [conv_layer(actns[i], actns[i+1], kernel_szs[i], stride=strides[i])\n",
    "        for i in range(len(strides))]\n",
    "    layers.append(nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten()))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.289376</td>\n",
       "      <td>0.248056</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = simple_cnn_avg((3,16,16,2))\n",
    "learner = Learner(data, model, metrics=[accuracy])\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将尝试使用AdaptiveConcatPool2d将它们连接起来。 事实上，我们会看到它提高了我们的准确性并大大减少了我们的损失！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(actns:Collection[int], kernel_szs:Collection[int]=None,\n",
    "               strides:Collection[int]=None) -> nn.Sequential:\n",
    "    \"CNN with `conv2d_relu` layers defined by `actns`, `kernel_szs` and `strides`\"\n",
    "    nl = len(actns)-1\n",
    "    kernel_szs = ifnone(kernel_szs, [3]*nl)\n",
    "    strides    = ifnone(strides   , [2]*nl)\n",
    "    layers = [conv_layer(actns[i], actns[i+1], kernel_szs[i], stride=strides[i])\n",
    "        for i in range(len(strides))]\n",
    "    layers.append(nn.Sequential(AdaptiveConcatPool2d(1), Flatten()))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.142278</td>\n",
       "      <td>0.091850</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = simple_cnn((3,16,16,2))\n",
    "learner = Learner(data, model, metrics=[accuracy])\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>class Lambda</b>\n",
    "\n",
    "`Lambda(func:LambdaFunc) :: Module`\n",
    "\n",
    "```python\n",
    "class Lambda(nn.Module):\n",
    "    \"An easy way to create a pytorch layer for a simple `func`.\"\n",
    "    def __init__(self, func:LambdaFunc):\n",
    "        \"create a layer that simply calls `func` with `x`\"\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "```\n",
    "为简单的func创建pytorch层的简单方法。\n",
    "\n",
    "这对于在Sequential对象内部的网络中使用函数非常有用。 因此，例如，假设我们想要应用log_softmax损失，我们需要更改输出批次的形状以便能够使用此损失。 我们可以通过调用添加一个应用必要的形状变化的图层：\n",
    "\n",
    "Lambda（lambda x：x.view（x.size（0）， -  1））\n",
    "\n",
    "让我们看一个示例，说明添加此图层时输出的形状如何变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for xb, yb in data.train_dl:\n",
    "    out = (model(*[xb]))\n",
    "    print(out.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0),-1))\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for xb, yb in data.train_dl:\n",
    "    out = (model(*[xb]))\n",
    "    print(out.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>class Flatten</b>\n",
    "\n",
    "`Flatten(full:bool=False) :: Module`\n",
    "        \n",
    "```python\n",
    "class Flatten(nn.Module):\n",
    "    \"Flatten `x` to a single dimension, often used at the end of a model. `full` for rank-1 tensor\"\n",
    "    def __init__(self, full:bool=False):\n",
    "        super().__init__()\n",
    "        self.full = full\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1) if self.full else x.view(x.size(0), -1)\n",
    "\n",
    "```\n",
    "将x展平为单个维度，通常在模型的末尾使用。 完全为1级张量\n",
    "\n",
    "我们上面构建的函数实际上在我们的库中实现为Flatten。 我们可以看到它在运行时返回相同的大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Flatten(),\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for xb, yb in data.train_dl:\n",
    "    out = (model(*[xb]))\n",
    "    print(out.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PoolFlatten</b>\n",
    "\n",
    "`PoolFlatten() → Sequential`\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "将nn.AdaptiveAvgPool2d应用于x，然后展平结果。\n",
    "\n",
    "我们可以使用PoolFlatten将这两个最终层（AdaptiveAvgPool2d和Flatten）组合在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    PoolFlatten()\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for xb, yb in data.train_dl:\n",
    "    out = (model(*[xb]))\n",
    "    print(out.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们给Lambda函数的另一个用途是当我们有一个期望输入不同于前一个的层时，使用ResizeBatch调整批量大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>class ResizeBatch</b>\n",
    "\n",
    "`ResizeBatch(*size:int) :: Module`\n",
    "        \n",
    "重塑x到大小，保持批量调暗相同的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1., -1.],\n",
      "         [ 1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1., -1.], [1., -1.]])[None]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., -1.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "out = ResizeBatch(4)\n",
    "print(out(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>class Debugger</b>\n",
    "\n",
    "`Debugger() :: Module`\n",
    "\n",
    "用于在模型内部进行调试的模块。\n",
    "\n",
    "调试器模块允许我们在培训期间窥视网络，并详细了解正在发生的事情。 我们可以在网络中的任何位置查看输入，输出和大小。\n",
    "\n",
    "例如，如果您运行以下内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='193', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/193 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> c:\\users\\yang\\appdata\\local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\fastai\\layers.py(219)forward()\n",
      "-> return x\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    Debugger(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    ")\n",
    "\n",
    "model.cuda()通过添加或连接密集= True的主题，将快捷方式与模块的结果合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>class PixelShuffle_ICNR</b>\n",
    "\n",
    "`PixelShuffle_ICNR(ni:int, nf:int=None, scale:int=2, blur:bool=False, norm_type=<NormType.Weight: 3>, leaky:float=None) :: Module`\n",
    "\n",
    "使用nn.PixelShuffle，`icnr` init和`weight_norm`按比例从`ni`过滤器到`nf`（默认`ni`）。\n",
    "\n",
    "<b>class MergeLayer</b>\n",
    "\n",
    "`MergeLayer(dense:bool=False) :: Module`\n",
    "\n",
    "通过添加或连接`dense=True`的主题，将快捷方式与模块的结果合并。\n",
    "\n",
    "<b>class PartialLayer</b>\n",
    "\n",
    "`PartialLayer(func, **kwargs) :: Module`\n",
    "\n",
    "应用部分（func，** kwargs）的图层。\n",
    "\n",
    "<b>class SigmoidRange</b>\n",
    "\n",
    "`SigmoidRange(low, high) :: Module`\n",
    "\n",
    "具有范围（低，x_max）的Sigmoid模块\n",
    "\n",
    "<b>class SequentialEx</b>\n",
    "\n",
    "`SequentialEx(*layers) :: Module`\n",
    "\n",
    "像nn.Sequential，但具有ModuleList语义，并且可以访问模块输入\n",
    "\n",
    "<b>class SelfAttention</b>\n",
    "\n",
    "`SelfAttention(n_channels:int) :: Module`\n",
    "\n",
    "自我关注层为nd。\n",
    "\n",
    "<b>class BatchNorm1dFlat</b>\n",
    "\n",
    "`BatchNorm1dFlat(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) :: BatchNorm1d`\n",
    "\n",
    "nn.BatchNorm1d，但首先展平领先尺寸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "<b>class FlattenedLoss</b>\n",
    "\n",
    "`FlattenedLoss(func, *args, axis:int=-1, floatify:bool=False, is_2d:bool=True, **kwargs)`\n",
    "\n",
    "与func相同，但会使输入和目标变平。\n",
    "\n",
    "使用args和kwargs创建一个func实例。 传递输出和目标时，它\n",
    "\n",
    "* 首先将轴放在输出中并使用转置设置目标\n",
    "* 将目标转换为float是floatify = True\n",
    "* 如果为_2d，则将输出压缩为二维，否则将一个维度挤压到一维\n",
    "* 应用了func的实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>BCEFlat</b>\n",
    "\n",
    "`BCEFlat(*args, axis:int=-1, floatify:bool=True, **kwargs)`\n",
    "\n",
    "与nn.BCELoss相同，但输入和目标变平。\n",
    "\n",
    "<b>BCEWithLogitsFlat</b>\n",
    "\n",
    "`BCEWithLogitsFlat(*args, axis:int=-1, floatify:bool=True, **kwargs)`\n",
    "\n",
    "与nn.BCEWithLogitsLoss相同，但会使输入和目标变平。\n",
    "\n",
    "<b>CrossEntropyFlat</b>\n",
    "\n",
    "`CrossEntropyFlat(*args, axis:int=-1, **kwargs)`\n",
    "\n",
    "与nn.CrossEntropyLoss相同，但会使输入和目标变平。\n",
    "\n",
    "<b>MSELossFlat</b>\n",
    "\n",
    "`MSELossFlat(*args, axis:int=-1, floatify:bool=True, **kwargs)`\n",
    "\n",
    "与nn.MSELoss相同，但会使输入和目标变平。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class NoopLoss\n",
    "\n",
    "`NoopLoss() :: Module`\n",
    "\n",
    "只返回输出的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class WassersteinLoss\n",
    "\n",
    "`WassersteinLoss() :: Module`\n",
    "\n",
    "For WGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to create modules\n",
    "<b>bn_drop_lin</b>\n",
    "\n",
    "`bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0.0, actn:Optional[Module]=None)`\n",
    "\n",
    "bn_drop_lin函数返回批量标准化，dropout和线性层的序列。 此自定义图层通常在模型的末尾使用。\n",
    "\n",
    "n_in表示输入大小的数量n_out输出的大小，bn我们是否需要批处理规范，p是多少dropout和actn是一个可选参数，用于在最后添加激活函数。\n",
    "\n",
    "<b>conv2d</b>\n",
    "\n",
    "`conv2d(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias=False, init:LayerFunc='kaiming_normal_') → Conv2d`\n",
    "\n",
    "创建并初始化nn.Conv2d图层。 padding默认为ks // 2。\n",
    "\n",
    "<b>conv2d_trans</b>\n",
    "\n",
    "`conv2d_trans(ni:int, nf:int, ks:int=2, stride:int=2, padding:int=0, bias=False) → ConvTranspose2d`\n",
    "\n",
    "创建nn.ConvTranspose2d图层。\n",
    "\n",
    "<b>conv_layer</b>\n",
    "\n",
    "`conv_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False, norm_type:Optional[NormType]=<NormType.Batch: 1>, use_activ:bool=True, leaky:float=None, transpose:bool=False, init:Callable='kaiming_normal_', self_attention:bool=False)`\n",
    "\n",
    "conv_layer函数返回nn.Conv2D，BatchNorm和ReLU或泄漏RELU激活函数的序列。\n",
    "\n",
    "n_in表示输入大小的数量n_out输出大小，ks内核大小，跨越我们想要应用卷积的步幅。 偏见将决定他们是否有偏见（如果是None，默认为True，除非使用batchnorm）。 norm_type选择规范化类型（或无）。 如果泄漏是无，则激活是标准ReLU，否则它是斜率泄漏的LeakyReLU。 最后，如果transpose = True，则卷积由ConvTranspose2D替换。\n",
    "\n",
    "<b>embedding</b>\n",
    "\n",
    "`embedding(ni:int, nf:int) → Module`\n",
    "\n",
    "创建一个输入大小为ni且输出大小为nf的嵌入层\n",
    "\n",
    "<b>relu</b>\n",
    "\n",
    "`relu(inplace:bool=False, leaky:float=None)`\n",
    "\n",
    "返回relu激活，可能是`leaky` 和 `nplace`.\n",
    "\n",
    "<b>res_block</b>\n",
    "\n",
    "`res_block(nf, dense:bool=False, norm_type:Optional[NormType]=<NormType.Batch: 1>, bottle:bool=False, **conv_kwargs)`\n",
    "\n",
    "Resnet块的nf功能。 conv_kwargs传递给conv_layer。\n",
    "\n",
    "<b>sigmoid_range</b>\n",
    "\n",
    "`sigmoid_range(x, low, high)`\n",
    "\n",
    "具有范围（低，高）的Sigmoid函数\n",
    "\n",
    "<b>simple_cnn</b>\n",
    "\n",
    "`simple_cnn(actns:Collection[int], kernel_szs:Collection[int]=None, strides:Collection[int]=None, bn=False) → Sequential`\n",
    "\n",
    "CNN，其中conv_layer由actns，kernel_szs和strides定义，加上batchnorm如果bn。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of modules\n",
    "<b>batchnorm_2d</b>\n",
    "\n",
    "`batchnorm_2d(nf:int, norm_type:NormType=<NormType.Batch: 1>)`\n",
    "\n",
    "具有nf特征的batchnorm2d层根据norm_type初始化。\n",
    "\n",
    "<b>icnr</b>\n",
    "\n",
    "`icnr(x, scale=2, init='kaiming_normal_')`\n",
    "\n",
    "ICNR init of x，具有scale和init功能。\n",
    "\n",
    "<b>trunc_normal_</b>\n",
    "\n",
    "`trunc_normal_(x:Tensor, mean:float=0.0, std:float=1.0) → Tensor`\n",
    "\n",
    "截断正常初始化。\n",
    "\n",
    "<b>icnr</b>\n",
    "\n",
    "`icnr(x, scale=2, init='kaiming_normal_')`\n",
    "\n",
    "ICNR init of x，具有scale和init功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NormType\n",
    "`Enum = [Batch, BatchZero, Weight, Spectral]`\n",
    "\n",
    "枚举。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
