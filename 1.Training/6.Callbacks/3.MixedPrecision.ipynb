{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合精确训练\n",
    "该模块允许神经网络的前向和后向传递以fp16（也称为半精度）完成。 如果您拥有带有张量内核的NVIDIA GPU，这一点尤为重要，因为它可以将您的培训速度提高200％或更多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "要以混合精度训练模型，您只需调用Learner.to_fp16，它会转换模型并修改现有的Learner以添加MixedPrecision。\n",
    "\n",
    "<b>to_fp16</b>\n",
    "\n",
    "`to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None, flat_master:bool=False, max_scale:float=16777216) → Learner`\n",
    "\n",
    "学习FP16精度模式。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.basics import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.146248</td>\n",
       "      <td>0.125436</td>\n",
       "      <td>0.956820</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "data = ImageDataBunch.from_folder(path)\n",
    "model = simple_cnn((3,16,16,2))\n",
    "learn = Learner(data, model, metrics=[accuracy]).to_fp16()\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关混合精确培训的详细信息，请参见NVIDIA的文档。 我们将在此总结基础知识。\n",
    "\n",
    "您可能想要调整的唯一参数是`loss_scale`。 这用于缩放损耗，因此它不会下溢fp16，导致精度损失（这在转换回fp32后的最终梯度计算中相反）。 通常，默认`512`运行良好。 您也可以使用`flat_master = True`启用或禁用主参数张量的展平，但在我们的测试中，差异可以忽略不计。\n",
    "\n",
    "在内部，回调确保所有模型参数（除了batchnorm图层，需要fp32）都转换为fp16，并且还保存了fp32副本。 fp32副本（主参数）用于使用优化器进行实际更新; fp16参数用于计算梯度。 这有助于避免学习率较低的下溢。\n",
    "\n",
    "所有这些都是通过以下Callback实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class MixedPrecision\n",
    "\n",
    "`MixedPrecision(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None, flat_master:bool=False, max_scale:float=16777216) :: LearnerCallback`\n",
    "\n",
    "用于为学习者创建回调的基类。\n",
    "\n",
    "### 回调方法\n",
    "您不必自己调用以下函数 -  fastai的Callback系统会自动调用它们来启用类的功能。\n",
    "\n",
    "<b>on_backward_begin</b>\n",
    "\n",
    "`on_backward_begin(last_loss:Rank0Tensor, **kwargs:Any) → Rank0Tensor`\n",
    "\n",
    "通过self.loss_scale缩放比例以防止下溢。\n",
    "\n",
    "<b>on_backward_end</b>\n",
    "\n",
    "`on_backward_end(**kwargs:Any)`\n",
    "\n",
    "将渐变转换回FP32并按比例划分它们。\n",
    "\n",
    "<b>on_loss_begin</b>\n",
    "\n",
    "`on_loss_begin(last_output:Tensor, **kwargs:Any) → Tensor`\n",
    "\n",
    "将半精度输出转换为FP32以避免减少溢出。\n",
    "\n",
    "<b>on_step_end</b>\n",
    "\n",
    "`on_step_end(**kwargs:Any)`\n",
    "\n",
    "更新从主模型到模型和零毕业的参数。\n",
    "\n",
    "<b>on_train_begin</b>\n",
    "\n",
    "`on_train_begin(**kwargs:Any)`\n",
    "\n",
    "准备主模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
